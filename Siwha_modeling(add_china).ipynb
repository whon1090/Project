{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from pandas import Series,DataFrame\n",
    "from math import *\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.formula.api as sm\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgunbd.ttf').get_name()\n",
    "mpl.rc('font', family=font_name)\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "## sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier, RandomForestRegressor, VotingClassifier, VotingRegressor)\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#LightGBM\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "\n",
    "# xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "import xgboost as xgb\n",
    "\n",
    "# randomforest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('D:/data/row_sihwa.csv',encoding='CP949')\n",
    "china = pd.read_csv('D:/data/chinapm_final.csv',encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site                 0\n",
       "date                 0\n",
       "so2                566\n",
       "co                 487\n",
       "o3                 448\n",
       "no2                439\n",
       "pm10               672\n",
       "pm25              8002\n",
       "weather            403\n",
       "wind_speed         351\n",
       "wind_direction     351\n",
       "rain_yn            384\n",
       "humid              333\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df = df[['site', 'date', 'so2', 'co', 'o3', 'no2', 'pm10', 'pm25', 'weather',\n",
    "       'wind_speed', 'wind_direction', 'rain_yn', 'humid']]\n",
    "df = df.iloc[:, [0,1,2,3,4,5,6,7,8,9,10,11,12]]\n",
    "df.loc[df['weather'] <= -30, 'weather' ] = np.float(\"nan\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 파생변수 ymd, md, month, season ,weekday,\n",
    "def date_split(df) :\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M')\n",
    "    df['ymd'] = [d.date() for d in df['date']]\n",
    "    df['month'] = [d.month for d in df['date']]\n",
    "    df['time'] = [d.time() for d in df['date']]\n",
    "    df['time'] = df['time'].apply(lambda x: x.strftime('%H'))\n",
    "    df['md'] = df['ymd'].apply(lambda x: x.strftime('%m-%d'))\n",
    "    df['weekday'] =df['date'].apply(lambda x: x.strftime('%A'))\n",
    "def season(x):\n",
    "    if '05-31'>= x>='03-01':\n",
    "        return('봄')\n",
    "    elif '11-30' >= x >= '09-01':\n",
    "        return('가을')\n",
    "    elif '08-31' >=x >= '06-01':\n",
    "        return('여름')\n",
    "    else: return('겨울')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling Definition\n",
    "def average_modeling(models) :\n",
    "    for i,m in enumerate(models):\n",
    "        print(i, m.__class__)\n",
    "        m.fit(x_train, y_train)\n",
    "\n",
    "    models = sorted(models, key=lambda m: mean_squared_error(y_test, m.predict(x_test)))\n",
    "\n",
    "\n",
    "    y_preds = np.array([m.predict(x_test) for m in models]).T\n",
    "    y_preds_mean = y_preds.mean(axis=1)\n",
    "    df_predict = pd.DataFrame({'Actual': y_test, 'Predicted': y_preds_mean})\n",
    "    print(\"RMSE : {:.3f}\".format((mean_squared_error(y_test, y_preds_mean))**0.5))\n",
    "\n",
    "    y_pred_log = np.array([m.predict(x_test) for m in models]).T.dot(\n",
    "    np.linspace(1.0, 0.0, len(models))/sum(np.linspace(1.0, 0.0, len(models))))\n",
    "    df_predict = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_log})\n",
    "    print(\"RMSE : {:.3f}\".format((mean_squared_error(y_test, y_pred_log))**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_split(df)\n",
    "df = df.dropna()\n",
    "df['season'] = df['md'].apply(season)\n",
    "df = df[df['site']=='시화산단']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['site','md','ymd'],axis=1,inplace=True)\n",
    "china.drop(['Unnamed: 0'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column = df.columns.tolist()\n",
    "df_column.remove('date')\n",
    "df_column.extend(['PM2.5_24h','PM10','PM10_24h','PM2.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].astype(str)\n",
    "df_china = pd.merge(df,china, on=['date'],how='left')\n",
    "df_china.dropna(inplace=True)\n",
    "df_china.drop(['date'],axis=1,inplace=True)\n",
    "df_china = df_china[df_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set china pm2.5_24h, pm10_24h\n",
    "df_china.drop(['PM10','PM2.5'],axis=1,inplace=True)\n",
    "data = df_china.copy()\n",
    "data['pm25_window_24'] = data['pm25'].rolling(window=6).mean()\n",
    "data.dropna(inplace=True)\n",
    "random.seed(100)\n",
    "\n",
    "## x, y setting\n",
    "X = data.drop([\"pm25\",'pm10'], axis=1)\n",
    "Y = data[\"pm25\"]\n",
    "\n",
    "# one-hot encoding\n",
    "# object 변수만 해당\n",
    "for i in [\"season\",\"weekday\",'time']:\n",
    "    X[i] = pd.get_dummies(X[i])\n",
    "# trian / test split 0.8/ 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'sklearn.ensemble.forest.RandomForestRegressor'>\n",
      "1 <class 'sklearn.ensemble.forest.RandomForestRegressor'>\n",
      "2 <class 'sklearn.ensemble.forest.RandomForestRegressor'>\n",
      "3 <class 'sklearn.ensemble.forest.RandomForestRegressor'>\n",
      "4 <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>\n",
      "5 <class 'sklearn.kernel_ridge.KernelRidge'>\n",
      "6 <class 'sklearn.kernel_ridge.KernelRidge'>\n",
      "7 <class 'xgboost.sklearn.XGBRegressor'>\n",
      "8 <class 'xgboost.sklearn.XGBRegressor'>\n",
      "9 <class 'xgboost.sklearn.XGBRegressor'>\n",
      "10 <class 'xgboost.sklearn.XGBRegressor'>\n",
      "11 <class 'xgboost.sklearn.XGBRegressor'>\n",
      "12 <class 'xgboost.sklearn.XGBRegressor'>\n",
      "RMSE : 8.016\n",
      "RMSE : 8.028\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter tunning\n",
    "models = []\n",
    "models = [RandomForestRegressor(n_estimators=n, random_state =5) for n in [ 30, 50, 100, 200]]\n",
    "models+=[GradientBoostingRegressor(random_state =5)]\n",
    "models+=[KernelRidge(alpha=0.6, kernel='polynomial', degree=i, coef0=2.5) for i in range(3,5)]\n",
    "models +=[XGBRegressor(max_depth= 8,\n",
    "                       objective='reg:squarederror',\n",
    "                       min_child_weight=1.111, n_estimator= n, subsample=0.6, random_state =5) for n in [30,50,100,200,500,1000]]\n",
    "\n",
    "average_modeling(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/data/row_sihwa.csv',encoding='CP949')\n",
    "china = pd.read_csv('D:/data/chinapm_final.csv',encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site                 0\n",
       "date                 0\n",
       "so2                566\n",
       "co                 487\n",
       "o3                 448\n",
       "no2                439\n",
       "pm10               672\n",
       "pm25              8002\n",
       "weather            403\n",
       "wind_speed         351\n",
       "wind_direction     351\n",
       "rain_yn            384\n",
       "humid              333\n",
       "dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df = df[['site', 'date', 'so2', 'co', 'o3', 'no2', 'pm10', 'pm25', 'weather',\n",
    "       'wind_speed', 'wind_direction', 'rain_yn', 'humid']]\n",
    "df = df.iloc[:, [0,1,2,3,4,5,6,7,8,9,10,11,12]]\n",
    "df.loc[df['weather'] <= -30, 'weather' ] = np.float(\"nan\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA 위한 파생변수 ymd, md, month, season ,weekday, pm categori 추가\n",
    "def date_split(df) :\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M')\n",
    "    df['ymd'] = [d.date() for d in df['date']]\n",
    "    df['month'] = [d.month for d in df['date']]\n",
    "    df['time'] = [d.time() for d in df['date']]\n",
    "    df['time'] = df['time'].apply(lambda x: x.strftime('%H'))\n",
    "    df['md'] = df['ymd'].apply(lambda x: x.strftime('%m-%d'))\n",
    "    df['weekday'] =df['date'].apply(lambda x: x.strftime('%A'))\n",
    "def season(x):\n",
    "    if '05-31'>= x>='03-01':\n",
    "        return('봄')\n",
    "    elif '11-30' >= x >= '09-01':\n",
    "        return('가을')\n",
    "    elif '08-31' >=x >= '06-01':\n",
    "        return('여름')\n",
    "    else: return('겨울')\n",
    "\n",
    "def categorical_PM(x):\n",
    "    if 8>= x>=0:\n",
    "        return('0')\n",
    "    elif 15 >= x >= 9 :\n",
    "        return('1')\n",
    "    elif 20 >=x >= 16:\n",
    "        return('2')\n",
    "    elif 25 >=x >= 21:\n",
    "        return('3')\n",
    "    elif 37 >=x >= 26:\n",
    "        return('4')\n",
    "    elif 50 >=x >= 38:\n",
    "        return('5')\n",
    "    elif 75 >=x >= 51:\n",
    "        return('6')\n",
    "    else: return('7') \n",
    "    \n",
    "# categorical modeling definition\n",
    "\n",
    "def categorical_modeling(models):\n",
    "    votingC = VotingClassifier(estimators=[(f'{m}' , m) for m in models], voting='hard')\n",
    " \n",
    "    votingC = votingC.fit(x_train, y_train)\n",
    "\n",
    "    #예측 진행\n",
    "    y_pred = votingC.predict(x_test) \n",
    "\n",
    "    # ACC, F1-score check\n",
    "    print(\"ACC : \", accuracy_score(y_test, y_pred))\n",
    "    print(\"f1-score : \", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_split(df)\n",
    "df = df.dropna()\n",
    "df['season'] = df['md'].apply(season)\n",
    "df['PM_categorical'] = df['pm25'].apply(categorical_PM)\n",
    "df['PM_categorical'] = df['PM_categorical'].astype('category')\n",
    "df = df[df['site']=='시화산단']\n",
    "df.drop(['site','md','ymd'],axis=1,inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "china.drop(['Unnamed: 0'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column = df.columns.tolist()\n",
    "df_column.remove('date')\n",
    "df_column.extend(['PM2.5_24h','PM10','PM10_24h','PM2.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].astype(str)\n",
    "df_china = pd.merge(df,china, on=['date'],how='left')\n",
    "df_china.dropna(inplace=True)\n",
    "df_china.drop(['date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_china.drop(['PM10','PM2.5'],axis=1,inplace=True)\n",
    "data = df_china.copy()\n",
    "data['pm25_window_24'] = data['pm25'].rolling(window=6).mean()\n",
    "data['pm10_window_24'] = data['pm10'].rolling(window=6).mean()\n",
    "data.dropna(inplace=True)\n",
    "random.seed(100)\n",
    "\n",
    "## x, y setting\n",
    "X = data.drop([\"pm25\",'pm10','PM_categorical'], axis=1)\n",
    "Y = data[\"PM_categorical\"]\n",
    "\n",
    "# one-hot encoding\n",
    "# object 변수만 해당\n",
    "for i in [\"season\", \"weekday\",'time']:\n",
    "    X[i] = pd.get_dummies(X[i])\n",
    "    \n",
    "# trian / test split 0.8/ 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC :  0.6004291845493562\n",
      "f1-score :  0.6108693775541216\n"
     ]
    }
   ],
   "source": [
    "# add lightGBM\n",
    "models = [RandomForestClassifier(n_estimators=n,random_state =5) for n in [50, 100, 200, 600]]\n",
    "models+=[GradientBoostingClassifier(n_estimators=n_e, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   random_state =5) for n_e in [3000, 5000]]\n",
    "models+=[XGBClassifier(max_depth=12,n_estimators=n, random_state =5) for n in [50, 100,200,400,1000]]\n",
    "models += [LGBMClassifier( n_estimators=n, max_depth=8, reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            random_state=5)for n in [50,100,500,1000]]\n",
    "\n",
    "categorical_modeling(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(5, shuffle=True, random_state=5)\n",
    "cross_val_score(categorical_modeling(models), X, Y, scoring=\"accuracy\", cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "x_data = np.vstack([x_train,x_test])\n",
    "mas = MaxAbsScaler()\n",
    "n_x_data = mas.fit_transform(x_data)\n",
    "train_length = len(x_train)\n",
    "x_test = n_x_data[train_length :,:]\n",
    "x_train = n_x_data[0:train_length,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=5\n",
    "models = [\n",
    "            'RFC',\n",
    "            'GBC',\n",
    "            'XGB',\n",
    "            'LGBM'\n",
    "            ]\n",
    "clfs = [RandomForestClassifier(n_estimators=n,random_state =5) for n in [50, 100, 200, 600]]\n",
    "clfs+=[GradientBoostingClassifier(n_estimators=n_e, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   random_state =5) for n_e in [3000, 5000]]\n",
    "clfs+=[XGBClassifier(max_depth=12,n_estimators=n, random_state =5) for n in [50, 100,200,400,1000]]\n",
    "clfs += [LGBMClassifier( n_estimators=n, max_depth=8, reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            random_state=5)for n in [50,100,500,1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
      "                       warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "1 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
      "                       warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "2 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
      "                       warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "3 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
      "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
      "                       warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "4 GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.05, loss='deviance', max_depth=4,\n",
      "                           max_features='sqrt', max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=15, min_samples_split=10,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=5, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "5 GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.05, loss='deviance', max_depth=4,\n",
      "                           max_features='sqrt', max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=15, min_samples_split=10,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=5000,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=5, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "6 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
      "              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=5,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "7 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=5,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "8 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
      "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=5,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "9 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
      "              min_child_weight=1, missing=None, n_estimators=400, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=5,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "10 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
      "              min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=5,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "11 LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=8,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=5, reg_alpha=0.041545473, reg_lambda=0.0735294,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "12 LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=8,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=5, reg_alpha=0.041545473, reg_lambda=0.0735294,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "13 LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=8,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=500, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=5, reg_alpha=0.041545473, reg_lambda=0.0735294,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "14 LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=8,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=1000, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=5, reg_alpha=0.041545473, reg_lambda=0.0735294,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pred_train_models = []\n",
    "pred_test_models = []\n",
    "\n",
    "kfold = 3 # use a bigger number\n",
    "\n",
    "cv = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=seed)\n",
    "cvfolds = list(cv.split(x_train,y_train))\n",
    "\n",
    "for j,clf in enumerate(clfs):\n",
    "    print(j,clf)\n",
    "    dataset_test_j = 0 \n",
    "    dataset_train_j = np.zeros((x_train.shape[0],len(np.unique(y_train))))\n",
    "    for i,(train_index, test_index) in enumerate(cvfolds):\n",
    "        n_x_train, n_x_val = x_train[train_index], x_train[test_index]\n",
    "        n_y_train, n_y_val = y_train[train_index], y_train[test_index]\n",
    "        print('fold ' + str(i))        \n",
    "        clf.fit(n_x_train,n_y_train)\n",
    "        dataset_train_j[test_index,:] = clf.predict_proba(n_x_val)\n",
    "        dataset_test_j += clf.predict_proba(x_test)\n",
    "    pred_train_models.append(dataset_train_j)\n",
    "    pred_test_models.append(dataset_test_j/float(kfold))\n",
    "    \n",
    "pred_blend_train = np.hstack(pred_train_models)\n",
    "pred_blend_test = np.hstack(pred_test_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Blending results with a Logistic Regression ... \n",
      "The Best parameters of the blending model\n",
      "{'C': 1000, 'tol': 0.01}\n",
      "The best score:0.5826357587465121\n"
     ]
    }
   ],
   "source": [
    "print('\\Blending results with a Logistic Regression ... ')\n",
    "\n",
    "blendParams = {'C':[1000],'tol':[0.01]} # test more values in your local machine\n",
    "clf = GridSearchCV(LogisticRegression(solver='newton-cg', multi_class='multinomial'), blendParams, scoring='accuracy',\n",
    "                   refit='True', n_jobs=-1, cv=5)\n",
    "clf.fit(pred_blend_train,y_train)\n",
    "print('The Best parameters of the blending model\\n{}'.format(clf.best_params_))\n",
    "print('The best score:{}'.format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add China PM10, PM2.5 3 days after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
