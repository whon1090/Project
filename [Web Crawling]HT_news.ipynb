{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "\n",
    "#python defalut module\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime,timedelta\n",
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현대 트랜시스 뉴스 크롤링  \n",
    "> 네이버 뉴스 통합검색으로 진행\n",
    ">\n",
    "> 통합검색시 page, start date, end date 설정\n",
    ">\n",
    "> 네이버 기사 제목 + URL + 기사검색시 요약본 3가지를 추출\n",
    ">\n",
    "> 네이버 뉴스란 클릭하면 네이버 뉴스로만 들어가짐 해당 기사 긁어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "날짜 형식 yyyy.mm.dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_date:\n",
    "    def __init__(self, s_year, phone_number, e_mail, addr):\n",
    "        self.s_year = start_year\n",
    "        self.s_month = start_month\n",
    "        self.s_day = start_day\n",
    "        self.e_year = end_year\n",
    "        self.e_month = end_month\n",
    "        self.e_day = end_day\n",
    "        \n",
    "        \n",
    "def set_contact():\n",
    "    start_date = input(\"Name: \")\n",
    "    phone_number = input(\"Phone Number: \")\n",
    "    e_mail = input(\"E-mail: \")\n",
    "    addr = input(\"Address: \")\n",
    "    print(name, phone_number, e_mail, addr)\n",
    "\n",
    "\n",
    "def get_days_list(self):\n",
    "    days_list = []\n",
    "    day = datetime.date.today()\n",
    "    for i in range(0, self.days):\n",
    "        day = day - datetime.timedelta(days=1)\n",
    "        days_list.append(day.isoformat())\n",
    "    return days_list\n",
    "\n",
    "def today_date():\n",
    "    # 'yyyy-mm-mm' 형식 날짜 반환\n",
    "    now = datetime.date.today()\n",
    "    return now.isoformat()\n",
    "\n",
    "def date_preprocessing(soup, date_class) :\n",
    "    date = soup.find_all(\"div\",{\"class\":date_class})\n",
    "    # 날짜가 기록된 태그 저장\n",
    "    start_date = datetime.now() # 현재 날짜 저장\n",
    "\n",
    "    for j in date :\n",
    "        date_list = j.get_text() #날짜 추출\n",
    "\n",
    "        if '시간' in date_list : # 날짜가 n시간 전으로 표기될 경우 문자열 처리\n",
    "            date_find = re.findall(r'\\d+', date_list) \n",
    "            final_date = str(datetime.now() - timedelta(hours=int(date_find[0])))\n",
    "            total_date.append(final_date)\n",
    "\n",
    "        elif '분' in date_list : # 날짜가 n분 전으로 표기될 경우 문자열 처리\n",
    "            date_find = re.findall(r'\\d+', date_list)\n",
    "            final_date = str(datetime.now() - timedelta(minutes=int(date_find[0])))\n",
    "            total_date.append(final_date)\n",
    "\n",
    "        elif '일' in date_list: # 날짜가 n일 전으로 표기될 경우 문자열 처리\n",
    "            date_find = re.findall(r'\\d+', date_list)\n",
    "            final_date = str(datetime.now() - timedelta(days=int(date_find[0])))\n",
    "            total_date.append(final_date)    \n",
    "\n",
    "        elif bool(re.search(r'\\d{3,4}.\\d{1,2}.\\d{2}',date_list)) == True : # yyyy.mm.dd로 표기될 경우 처리\n",
    "            final_date = date_list.replace('.',\"-\")\n",
    "            total_date.append(final_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleCrawler(object):\n",
    "    def __init__(self):\n",
    "        self.categories = {'정치': 100, '경제': 101, '사회': 102, '생활문화': 103, '세계': 104, 'IT과학': 105, '오피니언': 110,\n",
    "                           'politics': 100, 'economy': 101, 'society': 102, 'living_culture': 103, 'world': 104, 'IT_science': 105, 'opinion': 110}\n",
    "        self.selected_categories = []\n",
    "        self.date = {'start_year': 0, 'start_month': 0, 'end_year': 0, 'end_month': 0}\n",
    "        self.user_operating_system = str(platform.system())\n",
    "\n",
    "    def set_category(self, *args):\n",
    "        for key in args:\n",
    "            if self.categories.get(key) is None:\n",
    "                raise InvalidCategory(key)\n",
    "        self.selected_categories = args\n",
    "\n",
    "    def set_date_range(self, start_year, start_month, end_year, end_month):\n",
    "        args = [start_year, start_month, end_year, end_month]\n",
    "        if start_year > end_year:\n",
    "            raise InvalidYear(start_year, end_year)\n",
    "        if start_month < 1 or start_month > 12:\n",
    "            raise InvalidMonth(start_month)\n",
    "        if end_month < 1 or end_month > 12:\n",
    "            raise InvalidMonth(end_month)\n",
    "        if start_year == end_year and start_month > end_month:\n",
    "            raise OverbalanceMonth(start_month, end_month)\n",
    "        for key, date in zip(self.date, args):\n",
    "            self.date[key] = date\n",
    "        print(self.date)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_news_page_url(category_url, start_year, end_year, start_month, end_month):\n",
    "        made_urls = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            if start_year == end_year:\n",
    "                year_startmonth = start_month\n",
    "                year_endmonth = end_month\n",
    "            else:\n",
    "                if year == start_year:\n",
    "                    year_startmonth = start_month\n",
    "                    year_endmonth = 12\n",
    "                elif year == end_year:\n",
    "                    year_startmonth = 1\n",
    "                    year_endmonth = end_month\n",
    "                else:\n",
    "                    year_startmonth = 1\n",
    "                    year_endmonth = 12\n",
    "            \n",
    "            for month in range(year_startmonth, year_endmonth + 1):\n",
    "                for month_day in range(1, calendar.monthrange(year, month)[1] + 1):\n",
    "                    if len(str(month)) == 1:\n",
    "                        month = \"0\" + str(month)\n",
    "                    if len(str(month_day)) == 1:\n",
    "                        month_day = \"0\" + str(month_day)\n",
    "                        \n",
    "                    # 날짜별로 Page Url 생성\n",
    "                    url = category_url + str(year) + str(month) + str(month_day)\n",
    "\n",
    "                    # totalpage는 네이버 페이지 구조를 이용해서 page=10000으로 지정해 totalpage를 알아냄\n",
    "                    # page=10000을 입력할 경우 페이지가 존재하지 않기 때문에 page=totalpage로 이동 됨 (Redirect)\n",
    "                    totalpage = ArticleParser.find_news_totalpage(url + \"&page=10000\")\n",
    "                    for page in range(1, totalpage + 1):\n",
    "                        made_urls.append(url + \"&page=\" + str(page))\n",
    "        return made_urls\n",
    "\n",
    "    @staticmethod\n",
    "    def get_url_data(url, max_tries=10):\n",
    "        remaining_tries = int(max_tries)\n",
    "        while remaining_tries > 0:\n",
    "            try:\n",
    "                return requests.get(url)\n",
    "            except requests.exceptions:\n",
    "                sleep(60)\n",
    "            remaining_tries = remaining_tries - 1\n",
    "        raise ResponseTimeout()\n",
    "\n",
    "    def crawling(self, category_name):\n",
    "        # Multi Process PID\n",
    "        print(category_name + \" PID: \" + str(os.getpid()))    \n",
    "\n",
    "        writer = Writer(category_name=category_name, date=self.date)\n",
    "\n",
    "        # 기사 URL 형식\n",
    "        url = \"http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=\" + str(self.categories.get(category_name)) + \"&date=\"\n",
    "\n",
    "        # start_year년 start_month월 ~ end_year의 end_month 날짜까지 기사를 수집합니다.\n",
    "        day_urls = self.make_news_page_url(url, self.date['start_year'], self.date['end_year'], self.date['start_month'], self.date['end_month'])\n",
    "        print(category_name + \" Urls are generated\")\n",
    "        print(\"The crawler starts\")\n",
    "\n",
    "        for URL in day_urls:\n",
    "\n",
    "            regex = re.compile(\"date=(\\d+)\")\n",
    "            news_date = regex.findall(URL)[0]\n",
    "\n",
    "            request = self.get_url_data(URL)\n",
    "\n",
    "            document = BeautifulSoup(request.content, 'html.parser')\n",
    "\n",
    "            # html - newsflash_body - type06_headline, type06\n",
    "            # 각 페이지에 있는 기사들 가져오기\n",
    "            post_temp = document.select('.newsflash_body .type06_headline li dl')\n",
    "            post_temp.extend(document.select('.newsflash_body .type06 li dl'))\n",
    "            \n",
    "            # 각 페이지에 있는 기사들의 url 저장\n",
    "            post = []\n",
    "            for line in post_temp:\n",
    "                post.append(line.a.get('href')) # 해당되는 page에서 모든 기사들의 URL을 post 리스트에 넣음\n",
    "            del post_temp\n",
    "\n",
    "            for content_url in post:  # 기사 URL\n",
    "                # 크롤링 대기 시간\n",
    "                sleep(0.01)\n",
    "                \n",
    "                # 기사 HTML 가져옴\n",
    "                request_content = self.get_url_data(content_url)\n",
    "                try:\n",
    "                    document_content = BeautifulSoup(request_content.content, 'html.parser')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # 기사 제목 가져옴\n",
    "                    tag_headline = document_content.find_all('h3', {'id': 'articleTitle'}, {'class': 'tts_head'})\n",
    "                    text_headline = ''  # 뉴스 기사 제목 초기화\n",
    "                    text_headline = text_headline + ArticleParser.clear_headline(str(tag_headline[0].find_all(text=True)))\n",
    "                    if not text_headline:  # 공백일 경우 기사 제외 처리\n",
    "                        continue\n",
    "\n",
    "                    # 기사 본문 가져옴\n",
    "                    tag_content = document_content.find_all('div', {'id': 'articleBodyContents'})\n",
    "                    text_sentence = ''  # 뉴스 기사 본문 초기화\n",
    "                    text_sentence = text_sentence + ArticleParser.clear_content(str(tag_content[0].find_all(text=True)))\n",
    "                    if not text_sentence:  # 공백일 경우 기사 제외 처리\n",
    "                        continue\n",
    "\n",
    "                    # 기사 언론사 가져옴\n",
    "                    tag_company = document_content.find_all('meta', {'property': 'me2:category1'})\n",
    "                    text_company = ''  # 언론사 초기화\n",
    "                    text_company = text_company + str(tag_company[0].get('content'))\n",
    "                    if not text_company:  # 공백일 경우 기사 제외 처리\n",
    "                        continue\n",
    "                        \n",
    "                    # CSV 작성\n",
    "                    wcsv = writer.get_writer_csv()\n",
    "                    wcsv.writerow([news_date, category_name, text_company, text_headline, text_sentence, content_url])\n",
    "                    \n",
    "                    del text_company, text_sentence, text_headline\n",
    "                    del tag_company \n",
    "                    del tag_content, tag_headline\n",
    "                    del request_content, document_content\n",
    "\n",
    "                except Exception as ex:  # UnicodeEncodeError ..\n",
    "                    # wcsv.writerow([ex, content_url])\n",
    "                    del request_content, document_content\n",
    "                    pass\n",
    "        writer.close()\n",
    "\n",
    "    def start(self):\n",
    "        # MultiProcess 크롤링 시작\n",
    "        for category_name in self.selected_categories:\n",
    "            proc = Process(target=self.crawling, args=(category_name,))\n",
    "            proc.start()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Crawler = ArticleCrawler()\n",
    "    Crawler.set_category(\"생활문화\", \"IT과학\")\n",
    "    Crawler.set_date_range(2017, 1, 2018, 4)\n",
    "    Crawler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news",
   "language": "python",
   "name": "news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
